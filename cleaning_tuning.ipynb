{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RVq6toh55bid"
      },
      "source": [
        "### Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l3cK4u81VvYv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RvULyJWOntE",
        "outputId": "45b270e2-c181-493e-fe08-599b4833bb5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/alina/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /Users/alina/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import re\n",
        "from spellchecker import SpellChecker\n",
        "import pymystem3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ml0ynv8Ugn3x"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/Users/alina/Downloads/bug_data_15000.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NdWfMheuf5c8"
      },
      "source": [
        "### Simple data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "P6H_Rf-Lhl3J"
      },
      "outputs": [],
      "source": [
        "# making a copy of df to make changes and preprocess\n",
        "dff = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "5rvUre9vhsHn"
      },
      "outputs": [],
      "source": [
        "# flattening of target categories\n",
        "dff['Product_component'] = dff['Product'] + ' ' + dff['Component']\n",
        "dff = dff.drop(columns=['Product', 'Component'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "LUjVWlL1hzuG"
      },
      "outputs": [],
      "source": [
        "# drop rows where data in target and important columns is missing\n",
        "dff = dff.dropna(subset=['Product_component', 'Description'])\n",
        "\n",
        "# replace NaN values based on the mode (most frequent category of the column)\n",
        "mode_value = dff['Importance'].mode()[0]\n",
        "dff['Importance'].fillna(mode_value, inplace=True)\n",
        "\n",
        "# replace NaN values with a specific text\n",
        "dff['Title'].fillna('Unknown', inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kEVClorAV7BV"
      },
      "source": [
        "## data cleaning and preparation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BNXbADtpVv_e"
      },
      "source": [
        "### solving imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKolfi2-4ycc"
      },
      "outputs": [],
      "source": [
        "# function for dropping categories which contain less than 0.005% of the total amount of error reports\n",
        "# Calculate the percentage of each category\n",
        "category_percentages = dff[\"Product_component\"].value_counts() / dff[\"Product_component\"].count()\n",
        "\n",
        "# Create a list of categories to be dropped\n",
        "categories_to_drop = category_percentages[category_percentages < 0.005].index.tolist()\n",
        "\n",
        "# Select the rows where the \"Product\" column is in the list of categories to be dropped\n",
        "rows_to_drop = dff[dff[\"Product_component\"].isin(categories_to_drop)].index\n",
        "\n",
        "# Drop the selected rows\n",
        "dff.drop(rows_to_drop, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbuREaVx-TKp"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and testing sets\n",
        "\n",
        "X = dff[['Title', 'Description', 'Importance']]\n",
        "y = dff['Product_component']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A94XTscZvkkZ"
      },
      "outputs": [],
      "source": [
        "# Define preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text_title', CountVectorizer(), 'Title'),\n",
        "        ('text_desc', CountVectorizer(), 'Description'),\n",
        "        ('cat_importance', OneHotEncoder(handle_unknown='ignore'), ['Importance'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkzLaAyRvwA7"
      },
      "outputs": [],
      "source": [
        "# Fit and transform the training set\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# Transform the test set\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Encode the target feature\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt7H1b3Pvzkh"
      },
      "outputs": [],
      "source": [
        "# Define the pipeline with preprocessing and classifier\n",
        "pipeline = Pipeline([\n",
        "    ('clf', LogisticRegression())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "oWCBKNUSv22I",
        "outputId": "013e7d25-4267-49dc-fbb2-8d2746592478"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;clf&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;clf&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('clf', LogisticRegression())])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit the pipeline on training data\n",
        "pipeline.fit(X_train_processed, y_train_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2nQD9aavNC-",
        "outputId": "d0dcf102-3d5c-4a43-e7de-57c38caa85f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "      CDT cdt-core       0.75      0.14      0.24        21\n",
            "     CDT cdt-debug       0.88      0.47      0.61        15\n",
            "     CDT cdt-other       0.50      0.22      0.31        18\n",
            "          JDT Core       0.64      0.63      0.64       227\n",
            "         JDT Debug       0.68      0.69      0.68       254\n",
            "            JDT UI       0.67      0.79      0.73       631\n",
            "            PDE UI       0.81      0.54      0.65        93\n",
            "      Platform Ant       0.56      0.56      0.56        18\n",
            "  Platform Compare       0.67      0.39      0.49        41\n",
            "    Platform Debug       0.43      0.37      0.40        75\n",
            "Platform Resources       0.46      0.39      0.42       106\n",
            "      Platform SWT       0.72      0.65      0.68       251\n",
            "     Platform Team       0.81      0.86      0.83       306\n",
            "       Platform UI       0.66      0.67      0.67       510\n",
            "   Platform Update       0.69      0.74      0.71        57\n",
            "     Platform User       0.80      0.73      0.76        48\n",
            "\n",
            "          accuracy                           0.68      2671\n",
            "         macro avg       0.67      0.55      0.59      2671\n",
            "      weighted avg       0.68      0.68      0.68      2671\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test data\n",
        "y_pred = pipeline.predict(X_test_processed)\n",
        "\n",
        "# Decode the numerical labels back to their original form\n",
        "y_test_decoded = encoder.inverse_transform(y_test_encoded.reshape(-1, 1))\n",
        "y_pred_decoded = encoder.inverse_transform(y_pred.reshape(-1, 1))\n",
        "\n",
        "# Convert the arrays to pandas DataFrames\n",
        "y_test_decoded = pd.DataFrame(y_test_decoded)\n",
        "y_pred_decoded = pd.DataFrame(y_pred_decoded)\n",
        "\n",
        "# Replace NaN values with a default value\n",
        "default_value = 'Unknown'\n",
        "y_test_decoded = y_test_decoded.fillna(default_value)\n",
        "y_pred_decoded = y_pred_decoded.fillna(default_value)\n",
        "\n",
        "report = classification_report(y_test_decoded, y_pred_decoded)\n",
        "print('Classification Report:\\n', report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyHbjx4mWmDW"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cEPYxFOmWpoc"
      },
      "source": [
        "### regex cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2dM9NvbWnBz"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and testing sets\n",
        "\n",
        "X = dff[['Title', 'Description', 'Importance']]\n",
        "y = dff['Product_component']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvAJHgVBYV7B"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # lowercase the text\n",
        "    text = text.lower()\n",
        "    # remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # remove email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    # remove hashtags (keep the words without the '#')\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "    # remove special characters and punctuation\n",
        "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
        "    # remove timestamps\n",
        "    text = re.sub(r'\\(\\d{1,2}/\\d{1,2}/\\d{2} \\d{1,2}:\\d{2}:\\d{2} (AM|PM)\\)', ' ', text)\n",
        "    # remove numbers\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    # remove code snippets\n",
        "    # matches code snippets enclosed in triple backticks (```)\n",
        "    text = re.sub(r'```.+?```', ' ', text, flags=re.DOTALL)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMd-4D4sLHWp"
      },
      "outputs": [],
      "source": [
        "# Apply the preprocessing function to the text data\n",
        "X_train['Title'] = X_train['Title'].apply(preprocess_text)\n",
        "X_train['Description'] = X_train['Description'].apply(preprocess_text)\n",
        "X_test['Title'] = X_test['Title'].apply(preprocess_text)\n",
        "X_test['Description'] = X_test['Description'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvKE6IRcWnB1"
      },
      "outputs": [],
      "source": [
        "# Define preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text_title', CountVectorizer(), 'Title'),\n",
        "        ('text_desc', CountVectorizer(), 'Description'),\n",
        "        ('cat_importance', OneHotEncoder(handle_unknown='ignore'), ['Importance'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4F8DSXwWnB2"
      },
      "outputs": [],
      "source": [
        "# Fit and transform the training set\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# Transform the test set\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Encode the target feature\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylKqNR1LWnB4"
      },
      "outputs": [],
      "source": [
        "# Define the pipeline with preprocessing and classifier\n",
        "pipeline = Pipeline([\n",
        "    ('clf', LogisticRegression())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "K6qziO1FWnB7",
        "outputId": "2cbb7bab-76d8-4d63-d5da-9ea1df73471d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;clf&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;clf&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('clf', LogisticRegression())])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit the pipeline on training data\n",
        "pipeline.fit(X_train_processed, y_train_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDngTEITWnB9",
        "outputId": "dabcb535-8a3d-4e8f-b58e-93e8fc77f088"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "      CDT cdt-core       0.57      0.18      0.28        22\n",
            "     CDT cdt-debug       0.50      0.05      0.08        22\n",
            "       CDT cdt-doc       0.00      0.00      0.00         4\n",
            "     CDT cdt-other       0.14      0.07      0.10        14\n",
            " Equinox Incubator       0.00      0.00      0.00         3\n",
            "          JDT Core       0.69      0.62      0.65       226\n",
            "         JDT Debug       0.62      0.65      0.64       253\n",
            "           JDT Doc       0.00      0.00      0.00         2\n",
            "          JDT Text       0.00      0.00      0.00        18\n",
            "            JDT UI       0.65      0.76      0.70       629\n",
            "         PDE Build       0.33      0.25      0.29         4\n",
            "            PDE UI       0.60      0.53      0.56        89\n",
            "      Platform Ant       0.73      0.69      0.71        16\n",
            "      Platform CVS       0.00      0.00      0.00         1\n",
            "  Platform Compare       0.59      0.41      0.49        41\n",
            "    Platform Debug       0.32      0.24      0.27        74\n",
            "      Platform Doc       0.00      0.00      0.00         0\n",
            "   Platform Releng       0.00      0.00      0.00         5\n",
            "Platform Resources       0.46      0.31      0.37       102\n",
            "  Platform Runtime       0.00      0.00      0.00         1\n",
            "      Platform SWT       0.67      0.70      0.68       251\n",
            "Platform Scripting       0.00      0.00      0.00         7\n",
            "   Platform Search       0.00      0.00      0.00         4\n",
            "     Platform Team       0.76      0.85      0.80       294\n",
            "     Platform Text       0.00      0.00      0.00        13\n",
            "       Platform UI       0.65      0.69      0.67       539\n",
            "   Platform Update       0.82      0.62      0.70        65\n",
            "     Platform User       0.74      0.68      0.71        47\n",
            "   Platform WebDAV       0.00      0.00      0.00         4\n",
            "           Unknown       0.00      0.00      0.00         2\n",
            "\n",
            "          accuracy                           0.65      2752\n",
            "         macro avg       0.33      0.28      0.29      2752\n",
            "      weighted avg       0.63      0.65      0.63      2752\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test data\n",
        "y_pred = pipeline.predict(X_test_processed)\n",
        "\n",
        "# Decode the numerical labels back to their original form\n",
        "y_test_decoded = encoder.inverse_transform(y_test_encoded.reshape(-1, 1))\n",
        "y_pred_decoded = encoder.inverse_transform(y_pred.reshape(-1, 1))\n",
        "\n",
        "# Convert the arrays to pandas DataFrames\n",
        "y_test_decoded = pd.DataFrame(y_test_decoded)\n",
        "y_pred_decoded = pd.DataFrame(y_pred_decoded)\n",
        "\n",
        "# Replace NaN values with a default value\n",
        "default_value = 'Unknown'\n",
        "y_test_decoded = y_test_decoded.fillna(default_value)\n",
        "y_pred_decoded = y_pred_decoded.fillna(default_value)\n",
        "\n",
        "report = classification_report(y_test_decoded, y_pred_decoded)\n",
        "print('Classification Report:\\n', report)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XpNzNhNVaoz3"
      },
      "source": [
        "### spellcheck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAPIL098foXN"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and testing sets\n",
        "\n",
        "X = dff[['Title', 'Description', 'Importance']]\n",
        "y = dff['Product_component']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBccPpqMan9m"
      },
      "outputs": [],
      "source": [
        "# initialize the spell checker object\n",
        "spell = SpellChecker()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    try:\n",
        "        # Tokenize the text\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        \n",
        "        # Correct misspelled words\n",
        "        corrected_tokens = [spell.correction(token) for token in tokens]\n",
        "        \n",
        "        # Join the corrected tokens into a string\n",
        "        processed_text = \" \".join(corrected_tokens)\n",
        "        \n",
        "        return processed_text\n",
        "    except TypeError:\n",
        "        return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKa6YC7sqP1l"
      },
      "outputs": [],
      "source": [
        "# Apply the preprocessing function to the text data\n",
        "X_train['Title'] = X_train['Title'].apply(preprocess_text)\n",
        "X_train['Description'] = X_train['Description'].apply(preprocess_text)\n",
        "X_test['Title'] = X_test['Title'].apply(preprocess_text)\n",
        "X_test['Description'] = X_test['Description'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBLRSrcFqP1n"
      },
      "outputs": [],
      "source": [
        "# Define preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text_title', CountVectorizer(), 'Title'),\n",
        "        ('text_desc', CountVectorizer(), 'Description'),\n",
        "        ('cat_importance', OneHotEncoder(handle_unknown='ignore'), ['Importance'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-arXXTCqP1p"
      },
      "outputs": [],
      "source": [
        "# Fit and transform the training set\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# Transform the test set\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Encode the target feature\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53E8xAQ8qP1r"
      },
      "outputs": [],
      "source": [
        "# Define the pipeline with preprocessing and classifier\n",
        "pipeline = Pipeline([\n",
        "    ('clf', LogisticRegression())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXlP_vzfqP1v"
      },
      "outputs": [],
      "source": [
        "# Fit the pipeline on training data\n",
        "pipeline.fit(X_train_processed, y_train_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNRLE45PqP1y"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test data\n",
        "y_pred = pipeline.predict(X_test_processed)\n",
        "\n",
        "# Decode the numerical labels back to their original form\n",
        "y_test_decoded = encoder.inverse_transform(y_test_encoded.reshape(-1, 1))\n",
        "y_pred_decoded = encoder.inverse_transform(y_pred.reshape(-1, 1))\n",
        "\n",
        "# Convert the arrays to pandas DataFrames\n",
        "y_test_decoded = pd.DataFrame(y_test_decoded)\n",
        "y_pred_decoded = pd.DataFrame(y_pred_decoded)\n",
        "\n",
        "# Replace NaN values with a default value\n",
        "default_value = 'Unknown'\n",
        "y_test_decoded = y_test_decoded.fillna(default_value)\n",
        "y_pred_decoded = y_pred_decoded.fillna(default_value)\n",
        "\n",
        "report = classification_report(y_test_decoded, y_pred_decoded)\n",
        "print('Classification Report:\\n', report)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### stemming: Porter Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split the dataset into training and testing sets\n",
        "\n",
        "X = dff[['Title', 'Description', 'Importance']]\n",
        "y = dff['Product_component']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove the stopwords\n",
        "    tokens = [token for token in tokens]\n",
        "    # Stem the tokens\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the preprocessing function to the text data\n",
        "X_train['Title'] = X_train['Title'].apply(preprocess_text)\n",
        "X_train['Description'] = X_train['Description'].apply(preprocess_text)\n",
        "X_test['Title'] = X_test['Title'].apply(preprocess_text)\n",
        "X_test['Description'] = X_test['Description'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "      CDT cdt-core       0.50      0.18      0.27        22\n",
            "     CDT cdt-debug       1.00      0.05      0.09        22\n",
            "       CDT cdt-doc       0.00      0.00      0.00         4\n",
            "     CDT cdt-other       0.60      0.21      0.32        14\n",
            " Equinox Incubator       0.00      0.00      0.00         3\n",
            "          JDT Core       0.70      0.67      0.68       226\n",
            "         JDT Debug       0.63      0.68      0.65       253\n",
            "           JDT Doc       0.00      0.00      0.00         2\n",
            "          JDT Text       0.00      0.00      0.00        18\n",
            "            JDT UI       0.67      0.77      0.72       629\n",
            "         PDE Build       0.50      0.25      0.33         4\n",
            "            PDE UI       0.61      0.56      0.58        89\n",
            "      Platform Ant       0.87      0.81      0.84        16\n",
            "      Platform CVS       0.00      0.00      0.00         1\n",
            "  Platform Compare       0.67      0.54      0.59        41\n",
            "    Platform Debug       0.34      0.30      0.32        74\n",
            "      Platform Doc       0.00      0.00      0.00         0\n",
            "   Platform Releng       0.00      0.00      0.00         5\n",
            "Platform Resources       0.43      0.37      0.40       102\n",
            "  Platform Runtime       0.00      0.00      0.00         1\n",
            "      Platform SWT       0.64      0.68      0.66       251\n",
            "Platform Scripting       0.25      0.14      0.18         7\n",
            "   Platform Search       0.00      0.00      0.00         4\n",
            "     Platform Team       0.79      0.82      0.80       294\n",
            "     Platform Text       0.00      0.00      0.00        13\n",
            "       Platform UI       0.67      0.68      0.67       539\n",
            "   Platform Update       0.72      0.65      0.68        65\n",
            "     Platform User       0.70      0.68      0.69        47\n",
            "   Platform WebDAV       0.00      0.00      0.00         4\n",
            "           Unknown       0.00      0.00      0.00         2\n",
            "\n",
            "          accuracy                           0.66      2752\n",
            "         macro avg       0.38      0.30      0.32      2752\n",
            "      weighted avg       0.65      0.66      0.65      2752\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Define preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text_title', CountVectorizer(), 'Title'),\n",
        "        ('text_desc', CountVectorizer(), 'Description'),\n",
        "        ('cat_importance', OneHotEncoder(handle_unknown='ignore'), ['Importance'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Fit and transform the training set\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# Transform the test set\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Encode the target feature\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Define the pipeline with preprocessing and classifier\n",
        "pipeline = Pipeline([\n",
        "    ('clf', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on training data\n",
        "pipeline.fit(X_train_processed, y_train_encoded)\n",
        "\n",
        "# Evaluate on test data\n",
        "y_pred = pipeline.predict(X_test_processed)\n",
        "\n",
        "# Decode the numerical labels back to their original form\n",
        "y_test_decoded = encoder.inverse_transform(y_test_encoded.reshape(-1, 1))\n",
        "y_pred_decoded = encoder.inverse_transform(y_pred.reshape(-1, 1))\n",
        "\n",
        "# Convert the arrays to pandas DataFrames\n",
        "y_test_decoded = pd.DataFrame(y_test_decoded)\n",
        "y_pred_decoded = pd.DataFrame(y_pred_decoded)\n",
        "\n",
        "# Replace NaN values with a default value\n",
        "default_value = 'Unknown'\n",
        "y_test_decoded = y_test_decoded.fillna(default_value)\n",
        "y_pred_decoded = y_pred_decoded.fillna(default_value)\n",
        "\n",
        "report = classification_report(y_test_decoded, y_pred_decoded)\n",
        "print('Classification Report:\\n', report)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### stemming: SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# split the dataset into training and testing sets\n",
        "\n",
        "X = dff[['Title', 'Description', 'Importance']]\n",
        "y = dff['Product_component']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove the stopwords\n",
        "    tokens = [token for token in tokens]\n",
        "    # Stem the tokens\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the preprocessing function to the text data\n",
        "X_train['Title'] = X_train['Title'].apply(preprocess_text)\n",
        "X_train['Description'] = X_train['Description'].apply(preprocess_text)\n",
        "X_test['Title'] = X_test['Title'].apply(preprocess_text)\n",
        "X_test['Description'] = X_test['Description'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "      CDT cdt-core       0.50      0.23      0.31        22\n",
            "     CDT cdt-debug       0.50      0.09      0.15        22\n",
            "       CDT cdt-doc       0.00      0.00      0.00         4\n",
            "     CDT cdt-other       0.71      0.36      0.48        14\n",
            " Equinox Incubator       0.00      0.00      0.00         3\n",
            "          JDT Core       0.70      0.67      0.68       226\n",
            "         JDT Debug       0.62      0.65      0.63       253\n",
            "           JDT Doc       0.00      0.00      0.00         2\n",
            "          JDT Text       0.25      0.06      0.09        18\n",
            "            JDT UI       0.68      0.73      0.71       629\n",
            "         PDE Build       0.50      0.25      0.33         4\n",
            "            PDE UI       0.63      0.63      0.63        89\n",
            "      Platform Ant       0.87      0.81      0.84        16\n",
            "      Platform CVS       0.00      0.00      0.00         1\n",
            "  Platform Compare       0.65      0.54      0.59        41\n",
            "    Platform Debug       0.33      0.30      0.31        74\n",
            "      Platform Doc       0.00      0.00      0.00         0\n",
            "   Platform Releng       0.00      0.00      0.00         5\n",
            "Platform Resources       0.44      0.34      0.38       102\n",
            "  Platform Runtime       0.00      0.00      0.00         1\n",
            "      Platform SWT       0.61      0.72      0.66       251\n",
            "Platform Scripting       0.50      0.14      0.22         7\n",
            "   Platform Search       0.00      0.00      0.00         4\n",
            "     Platform Team       0.79      0.85      0.82       294\n",
            "     Platform Text       0.00      0.00      0.00        13\n",
            "       Platform UI       0.66      0.68      0.67       539\n",
            "   Platform Update       0.80      0.63      0.71        65\n",
            "     Platform User       0.70      0.66      0.68        47\n",
            "   Platform WebDAV       0.00      0.00      0.00         4\n",
            "           Unknown       0.00      0.00      0.00         2\n",
            "\n",
            "          accuracy                           0.66      2752\n",
            "         macro avg       0.38      0.31      0.33      2752\n",
            "      weighted avg       0.65      0.66      0.65      2752\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Define preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text_title', CountVectorizer(), 'Title'),\n",
        "        ('text_desc', CountVectorizer(), 'Description'),\n",
        "        ('cat_importance', OneHotEncoder(handle_unknown='ignore'), ['Importance'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Fit and transform the training set\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# Transform the test set\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Encode the target feature\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Define the pipeline with preprocessing and classifier\n",
        "pipeline = Pipeline([\n",
        "    ('clf', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on training data\n",
        "pipeline.fit(X_train_processed, y_train_encoded)\n",
        "\n",
        "# Evaluate on test data\n",
        "y_pred = pipeline.predict(X_test_processed)\n",
        "\n",
        "# Decode the numerical labels back to their original form\n",
        "y_test_decoded = encoder.inverse_transform(y_test_encoded.reshape(-1, 1))\n",
        "y_pred_decoded = encoder.inverse_transform(y_pred.reshape(-1, 1))\n",
        "\n",
        "# Convert the arrays to pandas DataFrames\n",
        "y_test_decoded = pd.DataFrame(y_test_decoded)\n",
        "y_pred_decoded = pd.DataFrame(y_pred_decoded)\n",
        "\n",
        "# Replace NaN values with a default value\n",
        "default_value = 'Unknown'\n",
        "y_test_decoded = y_test_decoded.fillna(default_value)\n",
        "y_pred_decoded = y_pred_decoded.fillna(default_value)\n",
        "\n",
        "report = classification_report(y_test_decoded, y_pred_decoded)\n",
        "print('Classification Report:\\n', report)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### lemmatization: WordnetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = dff[['Title', 'Description', 'Importance']]\n",
        "y = dff['Product_component']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/alina/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/alina/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "# Download the punkt tokenizer, and WordNet database\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define the preprocessing steps\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove the stopwords\n",
        "    tokens = [token for token in tokens]\n",
        "\n",
        "    # Lemmatize the tokens\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the preprocessing function to the text data\n",
        "X_train['Title'] = X_train['Title'].apply(preprocess_text)\n",
        "X_train['Description'] = X_train['Description'].apply(preprocess_text)\n",
        "X_test['Title'] = X_test['Title'].apply(preprocess_text)\n",
        "X_test['Description'] = X_test['Description'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "      CDT cdt-core       0.50      0.18      0.27        22\n",
            "     CDT cdt-debug       0.50      0.05      0.08        22\n",
            "       CDT cdt-doc       0.00      0.00      0.00         4\n",
            "     CDT cdt-other       0.62      0.36      0.45        14\n",
            " Equinox Incubator       0.00      0.00      0.00         3\n",
            "          JDT Core       0.69      0.66      0.68       226\n",
            "         JDT Debug       0.61      0.67      0.64       253\n",
            "           JDT Doc       0.00      0.00      0.00         2\n",
            "          JDT Text       0.17      0.06      0.08        18\n",
            "            JDT UI       0.69      0.76      0.72       629\n",
            "         PDE Build       0.33      0.25      0.29         4\n",
            "            PDE UI       0.58      0.61      0.59        89\n",
            "      Platform Ant       0.85      0.69      0.76        16\n",
            "      Platform CVS       0.00      0.00      0.00         1\n",
            "  Platform Compare       0.63      0.54      0.58        41\n",
            "    Platform Debug       0.34      0.31      0.33        74\n",
            "      Platform Doc       0.00      0.00      0.00         0\n",
            "   Platform Releng       0.00      0.00      0.00         5\n",
            "Platform Resources       0.45      0.36      0.40       102\n",
            "  Platform Runtime       0.00      0.00      0.00         1\n",
            "      Platform SWT       0.69      0.70      0.69       251\n",
            "Platform Scripting       0.20      0.14      0.17         7\n",
            "   Platform Search       0.00      0.00      0.00         4\n",
            "     Platform Team       0.81      0.81      0.81       294\n",
            "     Platform Text       0.00      0.00      0.00        13\n",
            "       Platform UI       0.67      0.71      0.69       539\n",
            "   Platform Update       0.81      0.66      0.73        65\n",
            "     Platform User       0.70      0.70      0.70        47\n",
            "   Platform WebDAV       0.00      0.00      0.00         4\n",
            "           Unknown       0.00      0.00      0.00         2\n",
            "\n",
            "          accuracy                           0.66      2752\n",
            "         macro avg       0.36      0.31      0.32      2752\n",
            "      weighted avg       0.65      0.66      0.66      2752\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Define preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text_title', CountVectorizer(), 'Title'),\n",
        "        ('text_desc', CountVectorizer(), 'Description'),\n",
        "        ('cat_importance', OneHotEncoder(handle_unknown='ignore'), ['Importance'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Fit and transform the training set\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# Transform the test set\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Encode the target feature\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Define the pipeline with preprocessing and classifier\n",
        "pipeline = Pipeline([\n",
        "    ('clf', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on training data\n",
        "pipeline.fit(X_train_processed, y_train_encoded)\n",
        "\n",
        "# Evaluate on test data\n",
        "y_pred = pipeline.predict(X_test_processed)\n",
        "\n",
        "# Decode the numerical labels back to their original form\n",
        "y_test_decoded = encoder.inverse_transform(y_test_encoded.reshape(-1, 1))\n",
        "y_pred_decoded = encoder.inverse_transform(y_pred.reshape(-1, 1))\n",
        "\n",
        "# Convert the arrays to pandas DataFrames\n",
        "y_test_decoded = pd.DataFrame(y_test_decoded)\n",
        "y_pred_decoded = pd.DataFrame(y_pred_decoded)\n",
        "\n",
        "# Replace NaN values with a default value\n",
        "default_value = 'Unknown'\n",
        "y_test_decoded = y_test_decoded.fillna(default_value)\n",
        "y_pred_decoded = y_pred_decoded.fillna(default_value)\n",
        "\n",
        "report = classification_report(y_test_decoded, y_pred_decoded)\n",
        "print('Classification Report:\\n', report)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### lemmatization: spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = dff[['Title', 'Description', 'Importance']]\n",
        "y = dff['Product_component']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/spacy/util.py:887: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.5.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define the preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Lemmatize the tokens\n",
        "    tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the preprocessing function to the text data\n",
        "X_train['Title'] = X_train['Title'].apply(preprocess_text)\n",
        "X_train['Description'] = X_train['Description'].apply(preprocess_text)\n",
        "X_test['Title'] = X_test['Title'].apply(preprocess_text)\n",
        "X_test['Description'] = X_test['Description'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "      CDT cdt-core       0.62      0.23      0.33        22\n",
            "     CDT cdt-debug       1.00      0.14      0.24        22\n",
            "       CDT cdt-doc       0.00      0.00      0.00         4\n",
            "     CDT cdt-other       0.60      0.21      0.32        14\n",
            " Equinox Incubator       0.00      0.00      0.00         3\n",
            "          JDT Core       0.65      0.64      0.65       226\n",
            "         JDT Debug       0.68      0.61      0.64       253\n",
            "           JDT Doc       0.00      0.00      0.00         2\n",
            "          JDT Text       0.10      0.06      0.07        18\n",
            "            JDT UI       0.68      0.76      0.72       629\n",
            "         PDE Build       0.33      0.25      0.29         4\n",
            "            PDE UI       0.62      0.58      0.60        89\n",
            "      Platform Ant       0.85      0.69      0.76        16\n",
            "      Platform CVS       0.00      0.00      0.00         1\n",
            "  Platform Compare       0.64      0.44      0.52        41\n",
            "    Platform Debug       0.34      0.38      0.36        74\n",
            "      Platform Doc       0.00      0.00      0.00         0\n",
            "   Platform Releng       0.00      0.00      0.00         5\n",
            "Platform Resources       0.47      0.38      0.42       102\n",
            "  Platform Runtime       0.00      0.00      0.00         1\n",
            "      Platform SWT       0.64      0.70      0.67       251\n",
            "Platform Scripting       0.33      0.14      0.20         7\n",
            "   Platform Search       0.00      0.00      0.00         4\n",
            "     Platform Team       0.82      0.82      0.82       294\n",
            "     Platform Text       0.00      0.00      0.00        13\n",
            "       Platform UI       0.64      0.70      0.67       539\n",
            "   Platform Update       0.81      0.65      0.72        65\n",
            "     Platform User       0.64      0.74      0.69        47\n",
            "   Platform WebDAV       0.00      0.00      0.00         4\n",
            "           Unknown       0.00      0.00      0.00         2\n",
            "\n",
            "          accuracy                           0.66      2752\n",
            "         macro avg       0.38      0.30      0.32      2752\n",
            "      weighted avg       0.65      0.66      0.65      2752\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Define preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text_title', CountVectorizer(), 'Title'),\n",
        "        ('text_desc', CountVectorizer(), 'Description'),\n",
        "        ('cat_importance', OneHotEncoder(handle_unknown='ignore'), ['Importance'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Fit and transform the training set\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# Transform the test set\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Encode the target feature\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Define the pipeline with preprocessing and classifier\n",
        "pipeline = Pipeline([\n",
        "    ('clf', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on training data\n",
        "pipeline.fit(X_train_processed, y_train_encoded)\n",
        "\n",
        "# Evaluate on test data\n",
        "y_pred = pipeline.predict(X_test_processed)\n",
        "\n",
        "# Decode the numerical labels back to their original form\n",
        "y_test_decoded = encoder.inverse_transform(y_test_encoded.reshape(-1, 1))\n",
        "y_pred_decoded = encoder.inverse_transform(y_pred.reshape(-1, 1))\n",
        "\n",
        "# Convert the arrays to pandas DataFrames\n",
        "y_test_decoded = pd.DataFrame(y_test_decoded)\n",
        "y_pred_decoded = pd.DataFrame(y_pred_decoded)\n",
        "\n",
        "# Replace NaN values with a default value\n",
        "default_value = 'Unknown'\n",
        "y_test_decoded = y_test_decoded.fillna(default_value)\n",
        "y_pred_decoded = y_pred_decoded.fillna(default_value)\n",
        "\n",
        "report = classification_report(y_test_decoded, y_pred_decoded)\n",
        "print('Classification Report:\\n', report)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### lemmatization: MyStem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pymystem3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = dff[['Title', 'Description', 'Importance']]\n",
        "y = dff['Product_component']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing mystem to /Users/alina/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-macosx.tar.gz\n"
          ]
        }
      ],
      "source": [
        "# Define the preprocessing steps\n",
        "mystem = pymystem3.Mystem()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove the stopwords\n",
        "    tokens = [token for token in tokens]\n",
        "\n",
        "    # Stem the tokens\n",
        "    tokens = [mystem.lemmatize(token)[0] for token in tokens]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the preprocessing function to the text data\n",
        "X_train['Title'] = X_train['Title'].apply(preprocess_text)\n",
        "X_train['Description'] = X_train['Description'].apply(preprocess_text)\n",
        "X_test['Title'] = X_test['Title'].apply(preprocess_text)\n",
        "X_test['Description'] = X_test['Description'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "      CDT cdt-core       0.62      0.23      0.33        22\n",
            "     CDT cdt-debug       1.00      0.14      0.24        22\n",
            "       CDT cdt-doc       0.00      0.00      0.00         4\n",
            "     CDT cdt-other       0.60      0.21      0.32        14\n",
            " Equinox Incubator       0.00      0.00      0.00         3\n",
            "          JDT Core       0.65      0.64      0.65       226\n",
            "         JDT Debug       0.68      0.61      0.64       253\n",
            "           JDT Doc       0.00      0.00      0.00         2\n",
            "          JDT Text       0.10      0.06      0.07        18\n",
            "            JDT UI       0.68      0.76      0.72       629\n",
            "         PDE Build       0.33      0.25      0.29         4\n",
            "            PDE UI       0.62      0.58      0.60        89\n",
            "      Platform Ant       0.85      0.69      0.76        16\n",
            "      Platform CVS       0.00      0.00      0.00         1\n",
            "  Platform Compare       0.64      0.44      0.52        41\n",
            "    Platform Debug       0.34      0.38      0.36        74\n",
            "      Platform Doc       0.00      0.00      0.00         0\n",
            "   Platform Releng       0.00      0.00      0.00         5\n",
            "Platform Resources       0.47      0.38      0.42       102\n",
            "  Platform Runtime       0.00      0.00      0.00         1\n",
            "      Platform SWT       0.64      0.70      0.67       251\n",
            "Platform Scripting       0.33      0.14      0.20         7\n",
            "   Platform Search       0.00      0.00      0.00         4\n",
            "     Platform Team       0.82      0.82      0.82       294\n",
            "     Platform Text       0.00      0.00      0.00        13\n",
            "       Platform UI       0.64      0.70      0.67       539\n",
            "   Platform Update       0.81      0.65      0.72        65\n",
            "     Platform User       0.64      0.74      0.69        47\n",
            "   Platform WebDAV       0.00      0.00      0.00         4\n",
            "           Unknown       0.00      0.00      0.00         2\n",
            "\n",
            "          accuracy                           0.66      2752\n",
            "         macro avg       0.38      0.30      0.32      2752\n",
            "      weighted avg       0.65      0.66      0.65      2752\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test data\n",
        "y_pred = pipeline.predict(X_test_processed)\n",
        "\n",
        "# Decode the numerical labels back to their original form\n",
        "y_test_decoded = encoder.inverse_transform(y_test_encoded.reshape(-1, 1))\n",
        "y_pred_decoded = encoder.inverse_transform(y_pred.reshape(-1, 1))\n",
        "\n",
        "# Convert the arrays to pandas DataFrames\n",
        "y_test_decoded = pd.DataFrame(y_test_decoded)\n",
        "y_pred_decoded = pd.DataFrame(y_pred_decoded)\n",
        "\n",
        "# Replace NaN values with a default value\n",
        "default_value = 'Unknown'\n",
        "y_test_decoded = y_test_decoded.fillna(default_value)\n",
        "y_pred_decoded = y_pred_decoded.fillna(default_value)\n",
        "\n",
        "report = classification_report(y_test_decoded, y_pred_decoded)\n",
        "print('Classification Report:\\n', report)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## mixed preparation techniques"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### solved imbalance + mystem lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function for dropping categories which contain less than 0.005% of the total amount of error reports\n",
        "# Calculate the percentage of each category\n",
        "category_percentages = dff[\"Product_component\"].value_counts() / dff[\"Product_component\"].count()\n",
        "\n",
        "# Create a list of categories to be dropped\n",
        "categories_to_drop = category_percentages[category_percentages < 0.005].index.tolist()\n",
        "\n",
        "# Select the rows where the \"Product\" column is in the list of categories to be dropped\n",
        "rows_to_drop = dff[dff[\"Product_component\"].isin(categories_to_drop)].index\n",
        "\n",
        "# Drop the selected rows\n",
        "dff.drop(rows_to_drop, inplace=True)\n",
        "\n",
        "# split the dataset into training and testing sets\n",
        "\n",
        "X = dff[['Title', 'Description', 'Importance']]\n",
        "y = dff['Product_component']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "      CDT cdt-core       0.62      0.23      0.33        22\n",
            "     CDT cdt-debug       1.00      0.14      0.24        22\n",
            "       CDT cdt-doc       0.00      0.00      0.00         4\n",
            "     CDT cdt-other       0.60      0.21      0.32        14\n",
            " Equinox Incubator       0.00      0.00      0.00         3\n",
            "          JDT Core       0.65      0.64      0.65       226\n",
            "         JDT Debug       0.68      0.61      0.64       253\n",
            "           JDT Doc       0.00      0.00      0.00         2\n",
            "          JDT Text       0.10      0.06      0.07        18\n",
            "            JDT UI       0.68      0.76      0.72       629\n",
            "         PDE Build       0.33      0.25      0.29         4\n",
            "            PDE UI       0.62      0.58      0.60        89\n",
            "      Platform Ant       0.85      0.69      0.76        16\n",
            "      Platform CVS       0.00      0.00      0.00         1\n",
            "  Platform Compare       0.64      0.44      0.52        41\n",
            "    Platform Debug       0.34      0.38      0.36        74\n",
            "      Platform Doc       0.00      0.00      0.00         0\n",
            "   Platform Releng       0.00      0.00      0.00         5\n",
            "Platform Resources       0.47      0.38      0.42       102\n",
            "  Platform Runtime       0.00      0.00      0.00         1\n",
            "      Platform SWT       0.64      0.70      0.67       251\n",
            "Platform Scripting       0.33      0.14      0.20         7\n",
            "   Platform Search       0.00      0.00      0.00         4\n",
            "     Platform Team       0.82      0.82      0.82       294\n",
            "     Platform Text       0.00      0.00      0.00        13\n",
            "       Platform UI       0.64      0.70      0.67       539\n",
            "   Platform Update       0.81      0.65      0.72        65\n",
            "     Platform User       0.64      0.74      0.69        47\n",
            "   Platform WebDAV       0.00      0.00      0.00         4\n",
            "           Unknown       0.00      0.00      0.00         2\n",
            "\n",
            "          accuracy                           0.66      2752\n",
            "         macro avg       0.38      0.30      0.32      2752\n",
            "      weighted avg       0.65      0.66      0.65      2752\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/alina/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Define the preprocessing steps\n",
        "mystem = pymystem3.Mystem()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove the stopwords\n",
        "    tokens = [token for token in tokens]\n",
        "\n",
        "    # Stem the tokens\n",
        "    tokens = [mystem.lemmatize(token)[0] for token in tokens]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the preprocessing function to the text data\n",
        "X_train['Title'] = X_train['Title'].apply(preprocess_text)\n",
        "X_train['Description'] = X_train['Description'].apply(preprocess_text)\n",
        "X_test['Title'] = X_test['Title'].apply(preprocess_text)\n",
        "X_test['Description'] = X_test['Description'].apply(preprocess_text)\n",
        "\n",
        "# Evaluate on test data\n",
        "y_pred = pipeline.predict(X_test_processed)\n",
        "\n",
        "# Decode the numerical labels back to their original form\n",
        "y_test_decoded = encoder.inverse_transform(y_test_encoded.reshape(-1, 1))\n",
        "y_pred_decoded = encoder.inverse_transform(y_pred.reshape(-1, 1))\n",
        "\n",
        "# Convert the arrays to pandas DataFrames\n",
        "y_test_decoded = pd.DataFrame(y_test_decoded)\n",
        "y_pred_decoded = pd.DataFrame(y_pred_decoded)\n",
        "\n",
        "# Replace NaN values with a default value\n",
        "default_value = 'Unknown'\n",
        "y_test_decoded = y_test_decoded.fillna(default_value)\n",
        "y_pred_decoded = y_pred_decoded.fillna(default_value)\n",
        "\n",
        "report = classification_report(y_test_decoded, y_pred_decoded)\n",
        "print('Classification Report:\\n', report)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## data preparation + TF-IDF vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split the dataset into training and testing sets\n",
        "\n",
        "X = dff[['Title', 'Description', 'Importance']]\n",
        "y = dff['Product_component']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# apply the preprocessing function to the text data\n",
        "X_train['Title'] = X_train['Title'].apply(preprocess_text)\n",
        "X_train['Description'] = X_train['Description'].apply(preprocess_text)\n",
        "X_test['Title'] = X_test['Title'].apply(preprocess_text)\n",
        "X_test['Description'] = X_test['Description'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text_title', CountVectorizer(), 'Title'),\n",
        "        ('text_desc', CountVectorizer(), 'Description'),\n",
        "        ('cat_importance', OneHotEncoder(handle_unknown='ignore'), ['Importance'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
